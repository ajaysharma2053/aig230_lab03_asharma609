{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91489c36",
   "metadata": {},
   "source": [
    "# AIG230 NLP (Week 3 Lab) â€” Notebook 2: Statistical Language Models (Train, Test, Evaluate)\n",
    "\n",
    "This notebook focuses on **n-gram Statistical Language Models (SLMs)**:\n",
    "- Train **unigram**, **bigram**, **trigram** models\n",
    "- Handle **OOV** with `<UNK>`\n",
    "- Apply **smoothing** (Add-k)\n",
    "- Evaluate with **cross-entropy** and **perplexity**\n",
    "- Do **next-word prediction** and simple **text generation**\n",
    "\n",
    "> Industry framing: even if modern systems use neural LMs, n-gram LMs are still useful for\n",
    "baselines, constrained domains, and for understanding evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a046e",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc4dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ajays\\desktop\\aig2ndsem\\naturallanguageprocessing\\lab3\\aig230_lab03_asharma609\\aig230-env\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ajays\\desktop\\aig2ndsem\\naturallanguageprocessing\\lab3\\aig230_lab03_asharma609\\aig230-env\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajays\\desktop\\aig2ndsem\\naturallanguageprocessing\\lab3\\aig230_lab03_asharma609\\aig230-env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ajays\\desktop\\aig2ndsem\\naturallanguageprocessing\\lab3\\aig230_lab03_asharma609\\aig230-env\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajays\\desktop\\aig2ndsem\\naturallanguageprocessing\\lab3\\aig230_lab03_asharma609\\aig230-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ee526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc428b4",
   "metadata": {},
   "source": [
    "## 1) Data: domain text you might see in real systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d207e4c",
   "metadata": {},
   "source": [
    "We use short texts that resemble:\n",
    "- release notes\n",
    "- incident summaries\n",
    "- operational runbooks\n",
    "- customer support messaging\n",
    "\n",
    "In practice, you would load thousands to millions of lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb34582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " 5,\n",
       " ['password reset email not received',\n",
       "  'printer not responding after driver update'],\n",
       " ['mobile app notifications delayed', 'wifi signal weak in conference room'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corpus = [\n",
    "    \"user cannot login after password change\",\n",
    "    \"vpn connection drops during peak hours\",\n",
    "    \"email attachments not downloading properly\",\n",
    "    \"mfa code expired before user could enter it\",\n",
    "    \"system performance slow after recent update\",\n",
    "    \"printer not responding after driver update\",\n",
    "    \"shared drive access denied for new employees\",\n",
    "    \"wifi signal weak in conference room\",\n",
    "    \"application crashes when uploading files\",\n",
    "    \"api authentication token expired unexpectedly\",\n",
    "    \"mailbox storage quota exceeded\",\n",
    "    \"remote desktop disconnects randomly\",\n",
    "    \"user account locked after multiple attempts\",\n",
    "    \"software installation fails due to permissions\",\n",
    "    \"network latency causing timeout errors\",\n",
    "    \"calendar events not syncing across devices\",\n",
    "    \"mobile app notifications delayed\",\n",
    "    \"database backup failed overnight\",\n",
    "    \"file upload stuck at ninety percent\",\n",
    "    \"password reset email not received\"\n",
    "]\n",
    "\n",
    "# Train/test split at sentence level\n",
    "random.seed(42)\n",
    "random.shuffle(corpus)\n",
    "split = int(0.75 * len(corpus))\n",
    "train_texts = corpus[:split]\n",
    "test_texts = corpus[split:]\n",
    "\n",
    "len(train_texts), len(test_texts), train_texts[:2], test_texts[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f9947",
   "metadata": {},
   "source": [
    "## 2) Tokenization + special tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c285f1d",
   "metadata": {},
   "source": [
    "We will:\n",
    "- lowercase\n",
    "- keep alphanumerics\n",
    "- split on whitespace\n",
    "- add sentence boundary tokens: `<s>` and `</s>`\n",
    "\n",
    "We will also map rare tokens to `<UNK>` based on training frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058f87da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " 'printer',\n",
       " 'driver',\n",
       " 'install',\n",
       " 'fails',\n",
       " 'with',\n",
       " 'error',\n",
       " '1603',\n",
       " '</s>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "def add_boundaries(tokens: List[str], n: int) -> List[str]:\n",
    "    # For n-grams, prepend (n-1) start tokens for simpler context handling\n",
    "    return [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(\"Printer driver install fails with error 1603\")\n",
    "add_boundaries(tokens, n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25308557",
   "metadata": {},
   "source": [
    "## 3) Build vocabulary and handle OOV with <UNK>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3338f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mobile', 'app', 'notifications', 'delayed'],\n",
       " ['<UNK>', '<UNK>', '<UNK>', '<UNK>'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Build vocab from training data\n",
    "train_tokens_flat = []\n",
    "for t in train_texts:\n",
    "    train_tokens_flat.extend(tokenize(t))\n",
    "\n",
    "freq = Counter(train_tokens_flat)\n",
    "\n",
    "# Typical practical rule: map tokens with frequency <= 1 to <UNK> in small corpora\n",
    "min_count = 3\n",
    "vocab = {w for w, c in freq.items() if c >= min_count}\n",
    "vocab |= {\"<UNK>\", \"<s>\", \"</s>\"}\n",
    "\n",
    "def replace_oov(tokens: List[str], vocab: set) -> List[str]:\n",
    "    return [tok if tok in vocab else \"<UNK>\" for tok in tokens]\n",
    "\n",
    "# Show OOV effect\n",
    "sample = tokenize(test_texts[0])\n",
    "sample, replace_oov(sample, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144f1a0",
   "metadata": {},
   "source": [
    "Increasing the min_count raises the OOV threshold, that causes more rare words to be replaced by <UNK>.\n",
    "This reduces the vocabulary size and sparsity, which can lower perplexity on small datasets by avoiding unseen n-grams.\n",
    "If the min_count is too high, excessive <UNK> replaces the useful lexical information, increasing perplexity again.\n",
    "So, there is a trade-off between vocab coverage and model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759444a0",
   "metadata": {},
   "source": [
    "## 4) Train n-gram counts (unigram, bigram, trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eac8fa",
   "metadata": {},
   "source": [
    "We will compute:\n",
    "- `ngram_counts[(w1,...,wn)]`\n",
    "- `context_counts[(w1,...,w_{n-1})]`\n",
    "\n",
    "Then probability:\n",
    "\\ndefault:  P(w_n | context) = count(context + w_n) / count(context)\n",
    "\n",
    "This fails when an n-gram is unseen, so we add smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33672bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def train_ngram_counts(texts: List[str], n: int, vocab: set) -> Dict[Tuple[str, ...], int]:\n",
    "    ngrams_counts = Counter()\n",
    "    context_counts = Counter()\n",
    "    for text in texts:\n",
    "        toks = replace_oov(tokenize(text), vocab)\n",
    "        toks = add_boundaries(toks, n)\n",
    "        for ng in get_ngrams(toks, n):\n",
    "            ngrams_counts[ng] += 1\n",
    "            context = ng[:-1]\n",
    "            context_counts[context] += 1\n",
    "\n",
    "    return ngrams_counts, context_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772a9032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<UNK>',): 74, ('</s>',): 15, ('not',): 4, ('after',): 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_counts, uni_ctx = train_ngram_counts(train_texts, n=1, vocab=vocab)\n",
    "uni_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b028b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<UNK>', '<UNK>'): 52,\n",
       "         ('<s>', '<UNK>'): 15,\n",
       "         ('<UNK>', '</s>'): 15,\n",
       "         ('<UNK>', 'not'): 4,\n",
       "         ('not', '<UNK>'): 4,\n",
       "         ('<UNK>', 'after'): 3,\n",
       "         ('after', '<UNK>'): 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_counts, bi_ctx = train_ngram_counts(train_texts, n=2, vocab=vocab)\n",
    "bi_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64cdaf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('<UNK>', '<UNK>', '<UNK>'): 33,\n",
       "         ('<s>', '<s>', '<UNK>'): 15,\n",
       "         ('<s>', '<UNK>', '<UNK>'): 14,\n",
       "         ('<UNK>', '<UNK>', '</s>'): 14,\n",
       "         ('<UNK>', 'not', '<UNK>'): 4,\n",
       "         ('<UNK>', '<UNK>', 'not'): 3,\n",
       "         ('<UNK>', 'after', '<UNK>'): 3,\n",
       "         ('after', '<UNK>', '<UNK>'): 3,\n",
       "         ('<UNK>', '<UNK>', 'after'): 2,\n",
       "         ('not', '<UNK>', '<UNK>'): 2,\n",
       "         ('not', '<UNK>', '</s>'): 1,\n",
       "         ('<s>', '<UNK>', 'not'): 1,\n",
       "         ('not', '<UNK>', 'after'): 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_counts, tri_ctx = train_ngram_counts(train_texts, n=3, vocab=vocab)\n",
    "tri_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8748f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram counts (for backoff)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "uni_counts = Counter()\n",
    "\n",
    "for sentence in train_texts:\n",
    "    tokens = tokenize(sentence)\n",
    "    tokens = [t if t in vocab else \"<UNK>\" for t in tokens]\n",
    "    tokens = add_boundaries(tokens, n=3)\n",
    "    uni_counts.update(tokens)\n",
    "\n",
    "# Unigram context is empty tuple\n",
    "uni_ctx = Counter()\n",
    "uni_ctx[()] = sum(uni_counts.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ba7c8",
   "metadata": {},
   "source": [
    "## 5) Add-k smoothing and probability function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed806986",
   "metadata": {},
   "source": [
    "Add-k smoothing (a common baseline):\n",
    "\\na) Add *k* to every possible next word count  \n",
    "b) Normalize by context_count + k * |V|\n",
    "\n",
    "P_k(w|h) = (count(h,w) + k) / (count(h) + k*|V|)\n",
    "\n",
    "Where V is the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de565994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_addk(ngram: Tuple[str, ...], ngram_counts: Counter, context_counts: Counter, V: int, k: float = 0.5) -> float :\n",
    "\n",
    "    \"\"\"Compute addk P(w_n | w_1 ... w_{n-1})\n",
    "    where ngram = (w_1, w_2, ..., w_n)\n",
    "    0 < k < = 1\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    context = ngram[:-1]\n",
    "    return (ngram_counts[ngram] + k)/(context_counts[context] + k*V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741851ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02857142857142857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(vocab)\n",
    "example = (\"<s>\", \"login\")\n",
    "prob_addk(example, bi_counts, bi_ctx, V, k=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69758252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_addk_prob(context, word, tri_counts, tri_ctx, V, k=0.5):\n",
    "    \"\"\"\n",
    "    Trigram add-k probability: P(word | w1, w2)\n",
    "    context = (w1, w2)\n",
    "    \"\"\"\n",
    "    trigram = (context[0], context[1], word)\n",
    "    return prob_addk(trigram, tri_counts, tri_ctx, V, k=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd730d2",
   "metadata": {},
   "source": [
    "## Backoff Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e27bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_prob(context2, word, k=0.5):\n",
    "    \"\"\"\n",
    "    Backoff language model probability:\n",
    "    - If trigram context (w1,w2) seen -> use trigram\n",
    "    - else fallback to bigram using (w2)\n",
    "    - else fallback to unigram\n",
    "\n",
    "    context2: tuple of last 2 tokens (w1, w2)\n",
    "    \"\"\"\n",
    "    V = len(vocab)\n",
    "\n",
    "    # --- 1) Try trigram ---\n",
    "    if tri_ctx.get(context2, 0) > 0:\n",
    "        return trigram_addk_prob(context2, word, tri_counts, tri_ctx, V, k=k)\n",
    "\n",
    "    # --- 2) Backoff to bigram (use last token only) ---\n",
    "    w2 = context2[-1]\n",
    "    context1 = (w2,)\n",
    "    if bi_ctx.get(context1, 0) > 0:\n",
    "        bigram = (w2, word)\n",
    "        return prob_addk(bigram, bi_counts, bi_ctx, V, k=k)\n",
    "\n",
    "    # --- 3) Backoff to unigram ---\n",
    "    unigram = (word,)\n",
    "    return prob_addk(unigram, uni_counts, uni_ctx, V, k=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6deec",
   "metadata": {},
   "source": [
    "## 6) Evaluate: cross-entropy and perplexity on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8426d9",
   "metadata": {},
   "source": [
    "We evaluate an LM by how well it predicts held-out text.\n",
    "\n",
    "Cross-entropy (average negative log probability):\n",
    "H = - (1/N) * sum log2 P(w_i | context)\n",
    "\n",
    "Perplexity:\n",
    "PP = 2^H\n",
    "\n",
    "Lower perplexity is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2d03099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy: 0.9262\n",
      "Perplexity: 1.9002\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_and_perplexity(test_texts, k=1.0):\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    V = len(vocab)  # vocab size (includes <UNK>, <s>, </s>)\n",
    "\n",
    "    for sentence in test_texts:\n",
    "        toks = tokenize(sentence)\n",
    "        toks = replace_oov(toks, vocab)          # IMPORTANT: consistent OOV handling\n",
    "        toks = add_boundaries(toks, n=3)         # trigram model => 2 start tokens\n",
    "\n",
    "        for i in range(2, len(toks)):\n",
    "            context = (toks[i-2], toks[i-1])\n",
    "            word = toks[i]\n",
    "\n",
    "            p = backoff_prob(context, word, k=k)\n",
    "\n",
    "            total_log_prob += math.log2(p)\n",
    "            total_tokens += 1\n",
    "\n",
    "    H = -total_log_prob / total_tokens\n",
    "    PP = 2 ** H\n",
    "    return H, PP\n",
    "\n",
    "\n",
    "H, PP = cross_entropy_and_perplexity(test_texts, k=1.0)\n",
    "print(f\"Cross-Entropy: {H:.4f}\")\n",
    "print(f\"Perplexity: {PP:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef338ce",
   "metadata": {},
   "source": [
    "## 7) Next-word prediction (top-k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d202f5",
   "metadata": {},
   "source": [
    "Given a context, compute the probability of each candidate next token and return the top-k.\n",
    "\n",
    "This mirrors:\n",
    "- autocomplete in constrained domains\n",
    "- template suggestion systems\n",
    "- command prediction in runbooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11363d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 next words for context ('password','reset'):\n",
      "       <UNK>  0.5965\n",
      "        </s>  0.2632\n",
      "         not  0.0702\n",
      "       after  0.0526\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(context_words, top_k=5, smoothing_k=1.0):\n",
    "    \"\"\"\n",
    "    context_words: tuple like (\"password\", \"reset\")\n",
    "    returns: list of (word, prob) sorted high->low\n",
    "    \"\"\"\n",
    "    V = len(vocab)\n",
    "\n",
    "    w1, w2 = context_words\n",
    "    w1 = w1.lower()\n",
    "    w2 = w2.lower()\n",
    "\n",
    "    # Make context consistent with training preprocessing\n",
    "    w1, w2 = replace_oov([w1, w2], vocab)\n",
    "    context = (w1, w2)\n",
    "\n",
    "    candidates = []\n",
    "    for word in vocab:\n",
    "        if word == \"<s>\":     # never predict start token\n",
    "            continue\n",
    "\n",
    "        p = trigram_addk_prob(context, word, tri_counts, tri_ctx, V, k=smoothing_k)\n",
    "        candidates.append((word, p))\n",
    "\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[:top_k]\n",
    "\n",
    "\n",
    "# Example\n",
    "print(\"Top-5 next words for context ('password','reset'):\")\n",
    "for w, p in predict_next_word((\"password\", \"reset\"), top_k=5, smoothing_k=1.0):\n",
    "    print(f\"{w:>12s}  {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672e1e9",
   "metadata": {},
   "source": [
    "## 8) Simple generation (bigram or trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd41fb",
   "metadata": {},
   "source": [
    "Text generation is not the main goal in SLMs, but it helps you verify:\n",
    "- boundary handling\n",
    "- smoothing\n",
    "- OOV decisions\n",
    "\n",
    "We will sample tokens until we hit `</s>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c8d1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAM:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "TRIGRAM: <UNK> <UNK>\n",
      "\n",
      "BIGRAM:  <UNK>\n",
      "TRIGRAM: <UNK> <UNK> <UNK>\n",
      "\n",
      "BIGRAM:  <UNK> <UNK>\n",
      "TRIGRAM: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "BIGRAM:  <UNK> <UNK> <UNK> not after after <UNK> <UNK> <UNK>\n",
      "TRIGRAM: <UNK> <UNK> <UNK> <UNK>\n",
      "\n",
      "BIGRAM:  <UNK> <UNK> <UNK> <UNK> <UNK> after <UNK> <UNK> <UNK> <UNK>\n",
      "TRIGRAM: <UNK> <UNK> <UNK>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_next(context_tokens: List[str], n: int, ngram_counts: Counter, context_counts: Counter, vocab: set, k_smooth: float = 0.5):\n",
    "    V = len(vocab)\n",
    "    context = tuple(context_tokens[-(n-1):]) if n > 1 else tuple()\n",
    "    words = [w for w in vocab if w != \"<s>\"]\n",
    "    probs = []\n",
    "    for w in words:\n",
    "        ng = context + (w,)\n",
    "        probs.append(prob_addk(ng, ngram_counts, context_counts, V, k=k_smooth))\n",
    "    # Normalize\n",
    "    s = sum(probs)\n",
    "    probs = [p/s for p in probs]\n",
    "    return random.choices(words, weights=probs, k=1)[0]\n",
    "\n",
    "def generate(n: int, ngram_counts: Counter, context_counts: Counter, vocab: set, max_len: int = 20, k_smooth: float = 0.5):\n",
    "    tokens = [\"<s>\"]*(n-1) if n > 1 else []\n",
    "    out = []\n",
    "    for _ in range(max_len):\n",
    "        w = sample_next(tokens, n, ngram_counts, context_counts, vocab, k_smooth=k_smooth)\n",
    "        if w == \"</s>\":\n",
    "            break\n",
    "        out.append(w)\n",
    "        tokens.append(w)\n",
    "    return \" \".join(out)\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"BIGRAM: \", generate(2, bi_counts, bi_ctx, vocab, max_len=18, k_smooth=0.5))\n",
    "    print(\"TRIGRAM:\", generate(3, tri_counts, tri_ctx, vocab, max_len=18, k_smooth=0.5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db5405",
   "metadata": {},
   "source": [
    "## 9) Model comparison: effect of n and smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7486afd",
   "metadata": {},
   "source": [
    "Try different `k` values. Notes:\n",
    "- `k=1.0` is Laplace smoothing (often too strong)\n",
    "- smaller `k` (like 0.1 to 0.5) is often better\n",
    "\n",
    "In real corpora, trigrams often beat bigrams, but require more data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb25609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram LM comparison (different add-k values)\n",
      "\n",
      "k = 1.0  | Cross-Entropy = 0.9262 | Perplexity = 1.9002\n",
      "k = 0.5  | Cross-Entropy = 0.8461 | Perplexity = 1.7976\n",
      "k = 0.1  | Cross-Entropy = 0.7646 | Perplexity = 1.6989\n"
     ]
    }
   ],
   "source": [
    "k_values = [1.0, 0.5, 0.1]\n",
    "\n",
    "print(\"Trigram LM comparison (different add-k values)\\n\")\n",
    "for k in k_values:\n",
    "    H, PP = cross_entropy_and_perplexity(test_texts, k=k)\n",
    "    print(f\"k = {k:<4} | Cross-Entropy = {H:.4f} | Perplexity = {PP:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e63d93ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 next words for phrase: 'user cannot'\n",
      "       <UNK>  0.596491\n",
      "        </s>  0.263158\n",
      "         not  0.070175\n",
      "       after  0.052632\n"
     ]
    }
   ],
   "source": [
    "def top_k_next_words_from_phrase(phrase: str, top_k: int = 5, smoothing_k: float = 1.0):\n",
    "    \"\"\"\n",
    "    Returns top-k next-word predictions for a phrase like: \"user cannot\"\n",
    "    Uses trigram if possible, otherwise falls back to bigram.\n",
    "    Assumes these already exist in your notebook:\n",
    "      tokenize, replace_oov, prob_addk, trigram_addk_prob,\n",
    "      vocab, bi_counts, bi_ctx, tri_counts, tri_ctx\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) tokenize the phrase the SAME way as training\n",
    "    tokens = tokenize(phrase)\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        raise ValueError(\"Phrase is empty after tokenization.\")\n",
    "\n",
    "    # 2) Decide which model we can use based on how many tokens we have\n",
    "    V = len(vocab)\n",
    "    candidates = []\n",
    "\n",
    "    # --- TRIGRAM (needs 2 context words) ---\n",
    "    if len(tokens) >= 2:\n",
    "        w1, w2 = tokens[-2].lower(), tokens[-1].lower()\n",
    "\n",
    "        # Make context consistent with training OOV handling\n",
    "        w1, w2 = replace_oov([w1, w2], vocab)\n",
    "        context = (w1, w2)\n",
    "\n",
    "        for word in vocab:\n",
    "            if word == \"<s>\":   # don't predict start token\n",
    "                continue\n",
    "            p = trigram_addk_prob(context, word, tri_counts, tri_ctx, V, k=smoothing_k)\n",
    "            candidates.append((word, p))\n",
    "\n",
    "    # --- BIGRAM fallback (only 1 context word) ---\n",
    "    else:\n",
    "        w1 = tokens[-1].lower()\n",
    "        w1 = replace_oov([w1], vocab)[0]\n",
    "        context = (w1,)\n",
    "\n",
    "        for word in vocab:\n",
    "            if word == \"<s>\":\n",
    "                continue\n",
    "            bigram = context + (word,)\n",
    "            p = prob_addk(bigram, bi_counts, bi_ctx, V, k=smoothing_k)\n",
    "            candidates.append((word, p))\n",
    "\n",
    "    # 3) Sort high -> low and return top-k\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[:top_k]\n",
    "\n",
    "\n",
    "# ===== Example (what the exercise asks) =====\n",
    "print(\"Top-5 next words for phrase: 'user cannot'\")\n",
    "for w, p in top_k_next_words_from_phrase(\"user cannot\", top_k=5, smoothing_k=1.0):\n",
    "    print(f\"{w:>12s}  {p:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941226b",
   "metadata": {},
   "source": [
    "Although the function requests top-5 predictions, only 4 tokens are returned because the start token \"s\" is explicitly excluded and the remaining vocabulary size is smaller than 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be49abe",
   "metadata": {},
   "source": [
    "## Exercises (do these during lab)\n",
    "1) Add 20 more realistic domain sentences to the corpus and re-run training/evaluation.  \n",
    "2) Change `min_count` (OOV threshold) and explain how perplexity changes.  \n",
    "3) Implement **backoff**: if a trigram is unseen, fall back to bigram; if unseen, fall back to unigram.  \n",
    "4) Create a function that returns **top-5 next words** given a phrase like: `\"user cannot\"`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aig230-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
